\subsection{External Memory Model}

% https://en.wikipedia.org/wiki/Random-access_machine

% TODO: go into more detail about the RAM model?
\alts[2]{
  {\alts{Establishing the basis of the, The} \gls{emm}, the widely used \gls{ram} model
  assumes \say{a \say{sufficiently} large uniform memory}~\cite[5]{afmh} with a constant
  access time.
  The},
  {The \gls{emm} is an extension of the \gls{ram} model.  While the latter assumes \say{a
  \say{sufficiently} large uniform memory}~\cite[5]{afmh} with a constant access time,
  the},
}
\gls{emm} \alts{divides, \alts{expands upon, extends} this by dividing} the memory into
\emph{internal} and \emph{external}.
\alts{The, Only the} internal memory is accessed directly, but its size is limited to
\(M\) \alts{items, words}.
The external memory is unbounded, but \alts{
  can only be accessed indirectly by loading data,
  data \x{is not accessed directly and} has to be loaded,
}
into internal memory \say{using \glslink{io}{I/Os} that move \(B\) contiguous
\alts{[items], words}}~\cite[5]{afmh}.

% > Although the word "I/O" suggests that external memory should be identified with disk
% > memory, we are free to choose any two levels of the memory hierarchy for internal and
% > external memory in the model.
%        -- Algorithms for Memory Hierarchies, page 6
The use of the term \emph{\acrshort{io}} here is somewhat non-standard.  While it suggests
\x{\alts{that, the}} external memory represents an \alts{
  {\gls{hdd} or \gls{ssd}},
  {\gls{hdd}, \gls{ssd} or similar},
}, it is
not constrained which \alts{physical storages, members of an actual memory hierarchy} are
\alts[2]{represented by, associated \alts{with, to}} \x{the} internal and external memory.
% Which members of an actual memory hierarchy are \alts{represented by, associated to} the
% internal and external memory is \alts{not constrained, as desired}.
If we choose the set of all CPU caches and the main memory, \(B\) becomes the cache line
size and \(M\) may be in the order of a few \si{\mebi\byte}.

The number of \acrshortpl{io} an algorithm requires in the \gls{emm} can augment the
information provided by standard asymptotic complexity analysis%
\x{, if \acrshort{io} is pivotal to an algorithm's runtime},
but \alts[2]{can't substitute, is no substitute for} measurements\x{~\cite[181]{afmh}}.
For example, the lower bound of \acrshortpl{io} needed for computing the sum of some
input of size \(N\), like in \cref{lst:ithare}, is
% See <http://erikdemaine.org/papers/BRICS2002/paper.pdf#page=7>.
\alts[3]{
  \(\lceil\nicefrac{N}{B}\rceil\),
  \(\left(\lfloor\nicefrac{N}{B}\rfloor + 2\right)\),
  \(\left(\lceil\nicefrac{N}{B}\rceil + 1\right)\),
}
in the \gls{emm}.\footnote{%
  TODO.
}
The linked list in that example probably takes almost \(N\) \acrshortpl{io},
though, since consecutive \alts{nodes, elements} are unlikely to fall into the same cache
line.  Thus, the predicted performance difference between \mintinline{cpp}{std::vector}
and \mintinline{cpp}{std::list} is at most \(B\), the number of items a cache line can
hold, which is \si{16} in this case.%
\footnote{%
  % TODO: elaborate on the size of `int`?
  A cache line is \alts{\si{64} bytes, \SI{64}{\byte}} on my laptop's CPU and an
  \mintinline{text}{int} \si{4} bytes with my compiler.
}
Recalling that the \alts{measured, actual} performance difference was
\num[round-mode=places, round-precision=0]{\speedup}, this is pretty inaccurate, but more
informative than saying both \x{algorithms'} data structure's time complexities for
traversal are \x{in} \(\Theta(N)\).

% An extensive repository of the theoretical lower bounds of \acrshortpl{io} needed in the
% \gls{emm} for many problems is available in the literature (for example \cite{afmh}).

Even with the simplifications made by the \gls{emm}, algorithmic analysis is usually only
done asymptotically: the number of \acrshortpl{io} is expressed \alts{in terms of, as}
\(\mathcal{O}\left(f\left(N, M, B\right)\right)\) or one of the related symbols.
Theoretical lower bounds of the \acrshortpl{io} needed in the \gls{emm} are available in
the literature (for example~\cite{afmh}) for many problems.

% This analysis only makes sense when \acrshort{io} is pivotal to an algorithm's
% efficiency.

% The objective is to \alts{
%   {design algorithms requiring a \alts{minimum number, minimum, minimal number} of
%   \acrshortpl{io}.},
%   {minimize the number of \alts{\acrshortpl{io}, I/Os} \alts{an algorithm needs,
%   needed}.},
% }

\x{
% Applying this model to \cref{lst:ithare} suggests
The \gls{emm} suggest that we can at best sum containers like in \cref{lst:ithare} using
\(\nicefrac{N}{B}\) \acrshortpl{io}, where \(N\) is the number of items to sum and \(B\)
the amount of them that fit in one block.
% Since there's 5000 containers of size 5000 and my cache line
% size is \SI{64}{B} and fits 8 \mintinline{cpp}{int} values:
% \begin{equation*}
%   N/B = 5000^2/8 = 3125000
% \end{equation*}
We can assume that the version with the list uses almost \(N\) \acrshortpl{io} because of
the random memory access.  Since a cache line fits 16 \mintinline{text}{int} values (\(B =
16\)) it
predicts a performance difference of 16.  This is not very accurate.
Analysis of \acrshortpl{io} needed by an algorithms \x{or data structure} may help inform
the decision of whether to consider it for empirical comparisons, though.
% that can be used prior to empirical comparisons.

% Apparently, the model is not practical for comparing these two data structures.
}

% Now for something completely different.
\x{
The \gls{emm} also suggests another approach to improve \cref{lst:ithare}, if we don't
want to \alts{trade, forgo, give up} the \x{asymptotically} constant time complexity of
insertions and deletions anywhere in the container: an \emph{unrolled list}.
}

% > In general it is best to hardcode cache line sizes at compile time by using the
% > `getconf` utility as in:
% >    gcc -DCLS=$(getconf LEVEL1_DCACHE_LINESIZE) ...
% > If the binaries are supposed to be generic, the largest cache line size should be
% > used.
%      -- Drepper, p. 50

% \subsubsection{Notes}
% \subsubsection{Caveats}
% \subsubsection{Limitations and Other Models}
\subsubsection{Limitations}

% Notes and potential problems:
% * The concept of I/Os that move B contiguous words directly translates to cache lines.
% * There's nothing equivalent to prefetching in the EMM.
% * > The main problem with hardware caches is that they use a fixed simplistic strategy
%   > for deciding which blocks are kept whereas the external memory model gives the
%   > programmer full control over the content of internal memory.
%        -- Algorithms for Memory Hierarchies, page 6
% * We lost associativity too.
% * And we don't model multiple levels of CPU caches, of course.

% Replacement policies.  Associativity.  More than two levels of memory.

% While the concept of \acrshortpl{io} directly models \alts{
%   most\footnote{%
%     TODO: not associativity though.
%   },
%   the
% }
% \alts{effects, existence} of cache lines,
% % are directly modeled by the concept of \acrshortpl{io},
% nothing in the \gls{emm} accounts for \x{those of} prefetching.

While the concept of \acrshortpl{io} directly models cache lines, most other
characteristics of memory hierarchies are ignored by the \gls{emm}.  For example:
\begin{itemize} % TODO: probably don't use a list structure.
  \item prefetching, or more generally the advantages of sequential access patterns,
  \item multi-level caches,
  \item the lack of direct control over the contents of caches, % (\alts{CPUs,
    % \glspl{cpu}} employ fixed, hardware-controlled replacement strategies)
  \item associativity,\footnote{%
      Associativity is not discussed in this paper; see \textcite{drepper2007} instead.
    }
  \item \gls{tlb}.
  % critical word stuff...
\end{itemize}
%
% > In the EMM it is generally assumed that the cost of I/Os is much greater than the cost
% > of computation, hence the design of the algorithm is usually motivated by the need to
% > minimise the number of I/Os.  However in the CMM and the IMM the relative miss
% > penalties are far smaller and we have to consider computation costs.
%      -- Algorithms for Memory Hierarchies, Section 8.6.4, page 188
%
% > The relative miss penalty is much lower for caches or the TLB than for disks, so
% > constant factors are important, we find that asymptotic analysis is not enough to
% > determine the performance of algorithms.
%      -- Algorithms for Memory Hierarchies, page 181
%
\alts{More fundamentally, Additionally}, the model's premise is that \acrshortpl{io} are
much more expensive than computation~\cite[188]{afmh}.
% This doesn't apply to data transfer between main memory and caches to the extent it does
% when accessing \glspl{hdd}.
While this is \alts{plausible, sensible, reasonable} when accessing \glspl{hdd}, it
doesn't apply to data transfer between main memory and caches to nearly the same extent.
%
Some of these shortcomings are addressed by refined machine models, which include more
details of real caches~\cite[178]{afmh}.
% I.e., CMM (cache memory model) and IMM (internal memory model).  There doesn't seem to
% be much literature about them [1].  Algorithms for Memory Hierarchies only cites a
% single paper that uses the IMM.  Don't discuss them.
%
% [1]: https://www.google.com/search?q=cache+memory+model+cmm
This complicates mathematical analysis~\cite[181]{afmh} and heuristics may be
used~\cite[191]{afmh}, which in turn \alts{exacerbates, amplifies} the need for
accompanying measurements~\cite[181]{afmh}.

\x{
\alts{
  {\alts[2]{On the contrary, Contrarily}, the \emph{\gls{com}} further increases the level
  of abstraction},
  {The opposite approach -- towards more abstraction -- is taken by the \gls{com}},
}.
}

% CMM and IMM add more detail ... bla bla bla ... COM actually abstracts further ...  bla
% bla ... counterintuitively this makes it more fitting in one aspect ... it works across
% the whole memory hierarchy ...

% vim: tw=90 sts=-1 sw=3 et fdm=marker
